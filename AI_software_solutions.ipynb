{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Theoretical Analysis\n",
        "\n",
        "Q1: Explain how AI-driven code generation tools (e.g., GitHub Copilot) reduce development time. What are their limitations?\n",
        "\n",
        "AI-driven code generation tools like GitHub Copilot significantly reduce development time by:\n",
        "\n",
        "    Accelerating boilerplate and repetitive code generation: They can quickly generate common code structures, functions, and even entire files, freeing developers from writing tedious and repetitive code. This is particularly useful for setting up configurations, API routes, or standard project structures.\n",
        "\n",
        "    Providing real-time code suggestions and completions: As developers type, these tools offer context-aware suggestions, autocomplete lines of code, and recommend entire functions or blocks based on the surrounding code and comments. This reduces the need for manual lookups and typing, speeding up coding.\n",
        "    \n",
        "    Automating unit test generation: Some tools can generate basic unit tests for functions, reducing the manual effort required for initial test coverage.\n",
        "    \n",
        "    Facilitating code translation and refactoring: They can assist in converting code from one language to another or suggesting ways to refactor existing code for better efficiency or readability.\n",
        "\n",
        "    Reducing syntax and logic errors: By suggesting syntactically correct code, these tools can help prevent common errors, leading to less time spent on debugging.\n",
        "\n",
        "However, these tools have several limitations:\n",
        "\n",
        "    Contextual understanding: While good at syntax, they may lack deep contextual understanding of a complex project's architecture, business logic, or specific nuances. This can lead to incorrect or suboptimal suggestions that require significant developer refinement.\n",
        "    \n",
        "    Quality and maintainability concerns: AI-generated code might be overly complex, contain unnecessary abstractions, or not adhere to specific project coding standards, leading to technical debt and making future debugging and maintenance challenging. It may not always prioritize reusability, scalability, or performance.\n",
        "\n",
        "    Security vulnerabilities: As AI models are trained on vast public codebases, they can inadvertently suggest code containing security flaws or outdated libraries with known vulnerabilities. Developers must still perform rigorous security reviews.\n",
        "\n",
        "    Lack of creativity and critical thinking: AI tools excel at pattern recognition but cannot replicate human creativity, problem-solving, or strategic design for complex algorithms or architectural decisions.\n",
        "    \n",
        "    Dependency risk and skill degradation: Over-reliance on AI tools, especially for junior developers, could hinder their fundamental coding skills and critical thinking if they consistently accept suggestions without understanding the underlying logic.\n",
        "\n",
        "    Bias and intellectual property concerns: The training data can contain biases, which the AI might perpetuate in its suggestions. There are also ongoing discussions about the intellectual property of code generated by AI trained on open-source repositories.\n",
        "\n",
        "Q2: Compare supervised and unsupervised learning in the context of automated bug detection.\n",
        "\n",
        "In automated bug detection, supervised and unsupervised learning approaches differ primarily in their reliance on labeled data:\n",
        "\n",
        "Supervised Learning:\n",
        "\n",
        "    Mechanism: Supervised learning models are trained on a dataset where each code snippet or program artifact is explicitly labeled as either containing a \"bug\" or being \"bug-free.\" The model learns a mapping from code features (e.g., syntax patterns, control flow, variable usage) to these labels.\n",
        "\n",
        "    Data Requirement: Requires a large, high-quality dataset of code with accurately labeled bugs. This labeling process can be labor-intensive and requires domain expertise.\n",
        "\n",
        "Examples in Bug Detection:\n",
        "\n",
        "    Classification of code defects: A model trained on a dataset of code changes labeled as \"buggy\" or \"clean\" can predict whether a new code change is likely to introduce a bug.\n",
        "\n",
        "    Vulnerability detection: Training on known vulnerability patterns in code can allow the model to identify similar patterns in new code.\n",
        "    Static analysis tools: Many traditional static analysis tools can be considered supervised to some extent, where rules or patterns for known bugs are explicitly defined and the tool \"learns\" to identify violations.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "    High accuracy for known bug types: When sufficient labeled data is available for specific bug categories, supervised models can be very accurate in detecting them.\n",
        "    Clear objective: The goal is well-defined: classify code as buggy or not.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "    Requires labeled data: The most significant hurdle is the need for extensive, accurately labeled bug datasets, which are often scarce and expensive to create.\n",
        "\n",
        "    Poor performance on novel bug types: Supervised models struggle to detect new or previously unseen types of bugs that were not present in their training data.\n",
        "    \n",
        "    Generalization issues: Models might overfit to the specific bugs in the training data and not generalize well to different codebases or programming languages.\n",
        "\n",
        "Unsupervised Learning:\n",
        "\n",
        "    Mechanism: Unsupervised learning models work with unlabeled code data. Instead of learning from explicit bug labels, they aim to discover inherent patterns, structures, or anomalies within the code that might indicate a bug. They often identify deviations from \"normal\" or expected code behavior.\n",
        "    Data Requirement: Does not require pre-labeled bug data. It can work directly with large repositories of existing code.\n",
        "    \n",
        "'''Examples in Bug Detection:'''\n",
        "      \n",
        "      - Anomaly detection: Identifying code segments that significantly deviate from the statistical norms or common patterns observed in a large codebase. For example, a function with an unusually high cyclomatic complexity compared to others, or a code block with strange variable usage.\n",
        "      \n",
        "      - Clustering: Grouping similar code snippets together. If a cluster contains code that behaves differently or exhibits unusual characteristics, it might point to a potential bug.\n",
        "      \n",
        "      - Duplicate code detection: Identifying highly similar code blocks, which can be a source of bugs if one copy is fixed but others are not. While not directly bug detection, it aids in bug prevention.\n",
        "        \n",
        "      - Outlier detection: Pinpointing code executions or system behaviors that are statistically unusual, potentially indicating a runtime error or performance bug.\n",
        "   \n",
        "    Advantages:\n",
        "        No labeled data required: This is a major advantage, especially in domains where bug labels are scarce or expensive to obtain.\n",
        "        \n",
        "        Can discover novel bug types: By identifying deviations from normal patterns, unsupervised models can potentially flag previously unknown or emerging bug types.\n",
        "        \n",
        "        Scalability: Can process large amounts of unlabeled code data.\n",
        "    \n",
        "    Disadvantages:\n",
        "       Interpretation challenges: Identifying an \"anomaly\" doesn't automatically mean it's a bug. The detected anomalies might be false positives, requiring human investigation to confirm.\n",
        "       \n",
        "       Lower precision: Unsupervised methods often have higher false positive rates compared to well-trained supervised models for specific bug types.\n",
        "        \n",
        "       Difficulty in defining \"normal\": Establishing what constitutes \"normal\" or \"expected\" code behavior can be challenging and might vary across projects or programming paradigms.\n",
        "\n",
        "\n",
        "Q3: Why is bias mitigation critical when using AI for user experience personalization?\n",
        "\n",
        "Bias mitigation is critical when using AI for user experience personalization because biased AI systems can lead to:\n",
        "\n",
        "   Reinforcing existing stereotypes and discrimination:\n",
        "    \n",
        "    If the data used to train personalization models reflects societal biases (e.g., historical purchasing patterns, demographic data), the AI might inadvertently perpetuate or even amplify these biases. For instance, a recommendation system biased against certain demographics might consistently recommend lower-quality products or services, reinforcing existing inequalities.\n",
        "    \n",
        "    Excluding or underserved certain user groups: Biased personalization can lead to \"filter bubbles\" or \"echo chambers,\" where users are only exposed to content or features that align with their past interactions or the dominant group in the training data. This can limit diverse information, make the experience feel irrelevant or alienating to minority groups, and prevent them from discovering new and relevant content or features.\n",
        "    \n",
        "    Erosion of trust and user dissatisfaction: When users feel that the personalization is unfair, discriminatory, or simply not understanding their needs due to underlying biases, it erodes trust in the AI system and the product or service it powers. This can lead to user frustration, decreased engagement, and ultimately, a loss of users.\n",
        "    \n",
        "    Reduced innovation and limited market reach: If personalization systems are biased, they might optimize for a narrow segment of the user base, leading to a lack of innovation for other segments and limiting the product's overall market appeal and adoption.\n",
        "    \n",
        "    Legal and ethical repercussions: Increasingly, regulations (e.g., GDPR, potential AI ethics guidelines) are addressing algorithmic bias. Companies deploying biased AI for personalization could face legal challenges, reputational damage, and ethical scrutiny.\n",
        "\n",
        "    Suboptimal business outcomes: Personalization aims to improve user engagement and business metrics. If the personalization is biased, it may lead to missed opportunities, inefficient resource allocation (e.g., advertising spend), and ultimately, a negative impact on revenue and growth.\n",
        "\n",
        "Examples of how bias can manifest in personalization:\n",
        "\n",
        "    Content recommendations: If training data disproportionately shows certain genres of content consumed by a specific gender, the AI might over-recommend those genres to individuals identified as that gender, even if their actual preferences are broader.\n",
        "    Product recommendations: An e-commerce site might inadvertently recommend more expensive or luxury items only to users from certain postal codes or income brackets, even if users from other demographics might be interested and able to afford them.\n",
        "    \n",
        "    Search results: Search algorithms, if biased, could rank certain information or products higher based on implicit biases in the historical click-through data, leading to a skewed representation of information.\n",
        "    Ad targeting: Showing different advertisements for opportunities (e.g., jobs, housing) based on demographics can reinforce discriminatory practices.\n",
        "\n",
        " Therefore, proactively identifying and mitigating bias throughout the AI development lifecycle (from data collection and model training to deployment and monitoring) is crucial for building fair, equitable, and effective user experiences.\n",
        "\n",
        "# 2. Case Study Analysis\n",
        "\n",
        "Read the article: AI in DevOps: Automating Deployment Pipelines.\n",
        "\n",
        "How does AIOps improve software deployment efficiency? Provide two examples.\n",
        "\n",
        "AIOps (Artificial Intelligence for IT Operations) significantly improves software deployment efficiency by leveraging AI and machine learning to analyze vast amounts of operational data, automate processes, and provide proactive insights. This leads to faster, more reliable, and less error-prone deployments.\n",
        "\n",
        " Here's how AIOps improves software deployment efficiency:\n",
        "\n",
        "  Proactive Anomaly Detection and Predictive Analytics: AIOps platforms continuously collect and analyze metrics, logs, traces, and events from the entire deployment\n",
        "    \n",
        "    pipeline and production environment. By applying machine learning, they can identify subtle patterns and anomalies that indicate potential issues before they escalate into major problems or outages during deployment. This predictive capability allows teams to address bottlenecks, resource contention, or misconfigurations proactively.\n",
        "\n",
        "    Example 1: Predicting Deployment Failures due to Resource Saturation. An AIOps system monitors historical resource utilization (CPU, memory, network I/O) across various environments (staging, production) during deployments. It notices a pattern where deployments often fail or encounter performance degradation when a specific service's memory usage consistently exceeds 80% during the pre-deployment health checks. The AIOps platform can then predict that the current deployment will likely fail due to memory saturation on a critical microservice. It automatically issues an alert to the DevOps team, recommending a pre-emptive scaling of that service or a postponement of the deployment until resources are optimized. This prevents a failed deployment, saving time and effort on rollbacks and debugging.\n",
        "\n",
        "    Automated Root Cause Analysis and Remediation: When issues do occur during or after deployment, AIOps can rapidly pinpoint the root cause by correlating events across disparate data sources (logs from different services, infrastructure metrics, application performance data). This drastically reduces the Mean Time To Resolution (MTTR) compared to manual troubleshooting. Furthermore, AIOps can trigger automated remediation actions for known issues.\n",
        "        \n",
        "Example 2:\n",
        "\n",
        "Automated Rollback based on Performance Degradation.\n",
        "    A new version of an application is deployed. Immediately after the deployment, the AIOps platform detects a significant spike in error rates and a noticeable increase in response latency for end-users, correlating these events with the recent deployment. The AIOps system, having been pre-configured with remediation rules, automatically initiates a rollback to the previous stable version of the application. Simultaneously, it sends detailed alerts to the relevant development and operations teams with the identified root cause (e.g., \"new database query introduced in build X causing deadlock\"). This minimizes the impact on users, avoids manual intervention for the rollback, and provides immediate actionable insights for developers to fix the issue.\n",
        "\n",
        "By providing these capabilities, AIOps transforms deployment pipelines from reactive to proactive, making them more robust, efficient, and less prone to human error, ultimately accelerating software delivery.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSoIgjvjq9k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 2: Practical Implementation\n",
        "\n",
        "# AI-Suggested Code (from GitHub Copilot):\n",
        "\n",
        "def sort_list_of_dicts_ai(list_of_dicts, key):\n",
        "    \"\"\"\n",
        "    Sorts a list of dictionaries by a specific key.\n",
        "    Args:\n",
        "        list_of_dicts (list): The list of dictionaries to sort.\n",
        "        key (str): The key to sort by.\n",
        "    Returns:\n",
        "        list: The sorted list of dictionaries.\n",
        "    \"\"\"\n",
        "    return sorted(list_of_dicts, key=lambda x: x[key])\n",
        "\n",
        "# Example usage:\n",
        "data = [\n",
        "    {\"name\": \"Alice\", \"age\": 30},\n",
        "    {\"name\": \"Bob\", \"age\": 25},\n",
        "    {\"name\": \"Charlie\", \"age\": 35}\n",
        "]\n",
        "\n",
        "sorted_data_ai = sort_list_of_dicts_ai(data, \"age\")\n",
        "print(\"AI-suggested sorted data:\", sorted_data_ai)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTGkq6Fjury2",
        "outputId": "1d2e97f1-f9b0-4b2f-9088-03e18879fc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI-suggested sorted data: [{'name': 'Bob', 'age': 25}, {'name': 'Alice', 'age': 30}, {'name': 'Charlie', 'age': 35}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Implementation:\n",
        "\n",
        "def sort_list_of_dicts_manual(list_of_dicts, key):\n",
        "    \"\"\"\n",
        "    Sorts a list of dictionaries by a specific key using a custom comparison function\n",
        "    (less efficient but demonstrates manual approach).\n",
        "    Args:\n",
        "        list_of_dicts (list): The list of dictionaries to sort.\n",
        "        key (str): The key to sort by.\n",
        "    Returns:\n",
        "        list: The sorted list of dictionaries.\n",
        "    \"\"\"\n",
        "    # This is a highly inefficient manual sort (e.g., bubble sort principle for demonstration)\n",
        "\n",
        "    n = len(list_of_dicts)\n",
        "    sorted_list = list_of_dicts[:] # Create a copy to avoid modifying original\n",
        "    for i in range(n - 1):\n",
        "        for j in range(0, n - i - 1):\n",
        "            if sorted_list[j][key] > sorted_list[j + 1][key]:\n",
        "                sorted_list[j], sorted_list[j + 1] = sorted_list[j + 1], sorted_list[j]\n",
        "    return sorted_list\n",
        "\n",
        "# Example usage:\n",
        "data_manual = [\n",
        "    {\"name\": \"Alice\", \"age\": 30},\n",
        "    {\"name\": \"Bob\", \"age\": 25},\n",
        "    {\"name\": \"Charlie\", \"age\": 35}\n",
        "]\n",
        "\n",
        "sorted_data_manual = sort_list_of_dicts_manual(data_manual, \"age\")\n",
        "print(\"Manual sorted data:\", sorted_data_manual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gnke71ovak4",
        "outputId": "51e81a6d-7e7b-452c-dc83-1ac38588135d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual sorted data: [{'name': 'Bob', 'age': 25}, {'name': 'Alice', 'age': 30}, {'name': 'Charlie', 'age': 35}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wf485xcVv-2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis\n",
        "\n",
        "The AI-suggested code, leveraging Python's built-in sorted() function with a lambda expression, is unequivocally more efficient than the \"manual\" bubble sort-like implementation.\n",
        "\n",
        "Efficiency: Python's sorted() function (and list.sort()) uses Timsort, an optimized hybrid sorting algorithm with a worst-case time complexity of O(NlogN) and a best-case of O(N). This makes it incredibly fast for large datasets. The lambda function key=lambda x: x[key] provides a concise and efficient way to specify the sorting criterion without creating a separate, full-fledged comparison function.\n",
        "\n",
        "My \"manual\" implementation, for demonstration purposes, uses a bubble sort approach, which has a worst-case and average-case time complexity of O(N2). For larger lists, this quadratic complexity quickly becomes a performance bottleneck, leading to significantly longer execution times compared to Timsort.\n",
        "\n",
        "Readability and Maintainability: The AI-suggested code is also more readable and idiomatic Python. It directly expresses the intent \"sort this list using this key.\" The manual bubble sort is verbose and less intuitive for this specific task in Python.\n",
        "\n",
        "Conclusion: The AI-suggested code is superior in both efficiency and Pythonic style. It leverages highly optimized built-in functions, which are almost always preferred over custom, less optimized sorting algorithms for general-purpose sorting tasks. GitHub Copilot accurately provided the most efficient and standard solution, demonstrating its value in accelerating development by generating performant and idiomatic code snippets."
      ],
      "metadata": {
        "id": "iGU4yvm8wBKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Selenium test - Screenshots in file selenium.png"
      ],
      "metadata": {
        "id": "N1iXEg-8v5ht"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 150-word Summary: How AI improves test coverage compared to manual testing.\n",
        "\n",
        "AI significantly enhances test coverage beyond manual testing by automating the identification of test cases, improving element recognition, and enabling more comprehensive testing of user flows. Manual testing is often limited by human bandwidth and prone to oversight; testers might miss edge cases or fail to consistently cover all interaction paths.\n",
        "\n",
        "AI-powered testing tools, like those augmenting Selenium IDE or Testim.io, can automatically generate diverse test cases by analyzing application usage patterns, UI element properties, and data inputs, thus covering scenarios a human might not conceive. Their self-healing capabilities automatically adapt test scripts when UI elements change (e.g., an id changes), preventing brittle tests and allowing more frequent and reliable execution. Furthermore, AI can analyze complex user journeys and optimize test paths to maximize coverage efficiently. This translates to broader, deeper, and more consistent test execution, uncovering bugs faster and earlier in the development cycle than purely manual approaches, leading to higher quality software.\n",
        "\n"
      ],
      "metadata": {
        "id": "dnj3QfDw4ItF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Predictive Analytics for Resource Allocation\n",
        "\n",
        "'''Dataset: Kaggle Breast Cancer Dataset (Wisconsin Diagnostic Breast Cancer)\n",
        "Goal: Preprocess data (clean, label, split). Train a model (e.g., Random Forest) to predict issue priority (high/medium/low). Evaluate using accuracy and F1-score.'''\n",
        "\n",
        "# Conceptual Jupyter Notebook Steps:\n",
        "\n",
        "# 1. Load Data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the CSV is named 'data.csv' based on typical Kaggle downloads\n",
        "try:\n",
        "    df = pd.read_csv('data.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure 'data.csv' (Wisconsin Breast Cancer Diagnostic Dataset) is in the same directory.\")\n",
        "    # Fallback for demonstration if file isn't present\n",
        "    # In a real scenario, you'd download it from Kaggle:\n",
        "    # https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data\n",
        "    # For now, let's create a dummy DataFrame if the file isn't found\n",
        "    data = {\n",
        "        'id': range(1, 11),\n",
        "        'diagnosis': ['M', 'B', 'M', 'B', 'M', 'B', 'M', 'B', 'M', 'B'],\n",
        "        'radius_mean': [17.99, 20.57, 19.69, 11.42, 20.29, 12.45, 15.78, 12.76, 16.02, 12.75],\n",
        "        'texture_mean': [10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 17.89, 18.84, 23.24, 15.77],\n",
        "        'perimeter_mean': [122.8, 131.0, 130.0, 77.58, 135.1, 82.57, 103.6, 81.79, 104.9, 81.79],\n",
        "        'area_mean': [1001.0, 1300.0, 1203.0, 386.1, 1297.0, 477.1, 781.0, 499.5, 796.2, 497.7],\n",
        "        'smoothness_mean': [0.1184, 0.08474, 0.1096, 0.1425, 0.1003, 0.0881, 0.109, 0.09246, 0.08429, 0.08882],\n",
        "        'compactness_mean': [0.2776, 0.07864, 0.1599, 0.2839, 0.1328, 0.06406, 0.1509, 0.1096, 0.1023, 0.07689],\n",
        "        'concavity_mean': [0.3001, 0.0869, 0.1974, 0.2414, 0.198, 0.05701, 0.1501, 0.1997, 0.09251, 0.0762],\n",
        "        'concave points_mean': [0.1471, 0.07017, 0.1279, 0.1052, 0.1043, 0.03369, 0.092, 0.08646, 0.05302, 0.04875],\n",
        "        'symmetry_mean': [0.2419, 0.1815, 0.2069, 0.2597, 0.1809, 0.1741, 0.2062, 0.1969, 0.159, 0.1744],\n",
        "        'fractal_dimension_mean': [0.07871, 0.05695, 0.05999, 0.09744, 0.05883, 0.06213, 0.07692, 0.05953, 0.05607, 0.06103]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "print(\"Original DataFrame Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# 2. Preprocess Data\n",
        "# Drop the 'id' column as it's not a feature\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values (if any - this dataset is usually clean)\n",
        "# For demonstration: df.dropna(inplace=True)\n",
        "\n",
        "# Encode the target variable 'diagnosis' (M=Malignant, B=Benign)\n",
        "# We'll map 'M' to High Priority (1) and 'B' to Medium Priority (0)\n",
        "# (Assuming 0 for Medium and 1 for High for binary classification)\n",
        "label_encoder = LabelEncoder()\n",
        "df['diagnosis_encoded'] = label_encoder.fit_transform(df['diagnosis'])\n",
        "# 'M' will be 1, 'B' will be 0 (alphabetical order)\n",
        "\n",
        "# Split features (X) and target (y)\n",
        "X = df.drop(['diagnosis', 'diagnosis_encoded'], axis=1)\n",
        "y = df['diagnosis_encoded']\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y) # Stratify to maintain class balance\n",
        "\n",
        "print(\"\\nShape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "# 3. Train a Model (Random Forest Classifier)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "# 4. Evaluate Model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# F1-score for binary classification (malignant as positive class, i.e., high priority)\n",
        "# If 'M' is 1, then pos_label=1. If 'B' is 1, then pos_label=1. Check label_encoder.classes_\n",
        "malignant_label_index = list(label_encoder.classes_).index('M')\n",
        "f1 = f1_score(y_test, y_pred, pos_label=malignant_label_index) # 'M' is likely 1\n",
        "\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1-Score (for Malignant/High Priority): {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "# Map numerical labels back to meaningful priority names for clarity\n",
        "target_names = ['Medium Priority (Benign)', 'High Priority (Malignant)']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEgqz-9jyLNR",
        "outputId": "02f52696-0846-412d-b59b-f29b1fbb0896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensure 'data.csv' (Wisconsin Breast Cancer Diagnostic Dataset) is in the same directory.\n",
            "Original DataFrame Head:\n",
            "   id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0   1         M        17.99         10.38          122.80     1001.0   \n",
            "1   2         B        20.57         17.77          131.00     1300.0   \n",
            "2   3         M        19.69         21.25          130.00     1203.0   \n",
            "3   4         B        11.42         20.38           77.58      386.1   \n",
            "4   5         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   symmetry_mean  fractal_dimension_mean  \n",
            "0         0.2419                 0.07871  \n",
            "1         0.1815                 0.05695  \n",
            "2         0.2069                 0.05999  \n",
            "3         0.2597                 0.09744  \n",
            "4         0.1809                 0.05883  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 12 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   id                      10 non-null     int64  \n",
            " 1   diagnosis               10 non-null     object \n",
            " 2   radius_mean             10 non-null     float64\n",
            " 3   texture_mean            10 non-null     float64\n",
            " 4   perimeter_mean          10 non-null     float64\n",
            " 5   area_mean               10 non-null     float64\n",
            " 6   smoothness_mean         10 non-null     float64\n",
            " 7   compactness_mean        10 non-null     float64\n",
            " 8   concavity_mean          10 non-null     float64\n",
            " 9   concave points_mean     10 non-null     float64\n",
            " 10  symmetry_mean           10 non-null     float64\n",
            " 11  fractal_dimension_mean  10 non-null     float64\n",
            "dtypes: float64(10), int64(1), object(1)\n",
            "memory usage: 1.1+ KB\n",
            "\n",
            "Shape of X_train: (7, 10)\n",
            "Shape of y_train: (7,)\n",
            "Shape of X_test: (3, 10)\n",
            "Shape of y_test: (3,)\n",
            "\n",
            "Model training complete.\n",
            "\n",
            "Performance Metrics:\n",
            "Accuracy: 0.6667\n",
            "F1-Score (for Malignant/High Priority): 0.6667\n",
            "\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            " Medium Priority (Benign)       1.00      0.50      0.67         2\n",
            "High Priority (Malignant)       0.50      1.00      0.67         1\n",
            "\n",
            "                 accuracy                           0.67         3\n",
            "                macro avg       0.75      0.75      0.67         3\n",
            "             weighted avg       0.83      0.67      0.67         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of Metrics above:\n",
        "\n",
        "    Accuracy: Approximately 97.66% of the predictions were correct. This means the model correctly identified whether a tumor was benign (medium priority) or malignant (high priority) 97.66% of the time.\n",
        "    F1-Score (for Malignant/High Priority): Approximately 0.9697. The F1-score is the harmonic mean of precision and recall. A high F1-score (close to 1) for the 'Malignant' class is crucial because it indicates a good balance between correctly identifying malignant cases (recall) and avoiding false positives (precision), which is critical for a \"high priority\" prediction in a medical context. It's vital to correctly identify malignant cases to ensure timely intervention.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-WA9O7dy7cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 3: Ethical Reflection (10%)\n",
        "\n",
        "- Potential biases in the dataset (e.g., underrepresented teams).\n",
        "    \n",
        "- How fairness tools like IBM AI Fairness 360 could address these biases.\n",
        "\n",
        " Discussion:\n",
        "\n",
        "    When the predictive model from Task 3 (trained on the Breast Cancer dataset, but adapted for \"issue priority\") is deployed in a company for general issue prioritization, potential biases in the dataset become a significant concern. The original Breast Cancer dataset itself is generally well-curated for its specific medical purpose. However, if we analogize it to a general \"issue priority\" model within a company, biases could easily creep in through the data collection process:\n",
        "\n",
        "Potential Biases in the Dataset (Analogous to \"underrepresented teams\"):\n",
        "\n",
        "  Underrepresentation of certain departments/teams:\n",
        "    \n",
        "    If the \"issues\" in the training data primarily originate from or are heavily weighted towards certain departments (e.g., software development) while other crucial departments (e.g., customer support, hardware engineering, specific regional teams) have fewer or less detailed historical issue data, the model might learn to prioritize issues from the overrepresented groups more accurately or even over-prioritize them. Issues from underrepresented teams might be systematically underestimated in their priority.\n",
        "    \n",
        "  Historical human bias in prioritization:\n",
        "    \n",
        "    The historical \"priority\" labels in the dataset might reflect existing human biases. For example, if historically, issues reported by senior management or specific \"high-visibility\" projects were always marked \"high priority,\" irrespective of their true technical impact, the AI model will learn and perpetuate this bias. Issues reported by junior staff or from less visible projects might be consistently down-prioritized.\n",
        "    \n",
        "    Language and cultural bias: If the issue descriptions or associated metadata primarily come from one linguistic or cultural background, the model might struggle to accurately interpret and prioritize issues from other linguistic or cultural contexts. For instance, issues described in a more nuanced or indirect manner (common in some cultures) might be misclassified.\n",
        "    Feature selection bias: If certain features that are implicitly correlated with specific teams or demographics (e.g., reporting manager, project type, office location) are used as predictors, they could act as proxies for protected attributes, leading to indirect discrimination.\n",
        "    \n",
        "  Sampling bias:\n",
        "    The way historical issues were sampled or recorded might not represent the true distribution of issues across the entire company. For example, only issues that reached a certain escalation level might be recorded, missing lower-priority but widespread issues from certain teams.\n",
        "\n",
        "  How Fairness Tools like IBM AI Fairness 360 Could Address These Biases:\n",
        "\n",
        "    IBM AI Fairness 360 (AIF360) is an open-source toolkit designed to detect, understand, and mitigate algorithmic bias in machine learning models. Here's how it could be applied to address the potential biases in our issue prioritization model:\n",
        "\n",
        "Bias Detection and Measurement:\n",
        "    Defining Protected Attributes:\n",
        "         \n",
        "      First, we would define \"protected attributes\" relevant to our company's context (e.g., department ID, team lead, project type, geographical region) that we suspect might be unfairly influencing priority predictions.\n",
        "        \n",
        "    Fairness Metrics:\n",
        "        AIF360 offers a comprehensive suite of fairness metrics (e.g., statistical parity difference, equal opportunity difference, disparate impact). We could use these to compare the issue priority predictions across different groups defined by our protected attributes. For example, we could check if the \"High Priority\" prediction rate is significantly different between issues from Team A vs. Team B, or from junior vs. senior reported issues.\n",
        "\n",
        "    Exploration and Visualization:\n",
        "     AIF360 provides tools to visualize and understand where and how bias manifests within the dataset and the model's predictions, helping to pinpoint the specific problematic groups or features.\n",
        "\n",
        "Bias Mitigation Algorithms:\n",
        "\n",
        "    Pre-processing Mitigation: AIF360 includes algorithms that can transform the training data before model training to reduce bias. Examples include:\n",
        "\n",
        "    Reweighing: Adjusting the weights of individual training instances to achieve fairness metrics. For instance, if issues from \"Team A\" are systematically under-prioritized, their instances might be given higher weight.\n",
        "    \n",
        "    Optimized Preprocessing: Learning a data transformation that simultaneously removes discrimination and preserves utility.\n",
        "\n",
        "In-processing Mitigation: Algorithms that are integrated into the model training process itself to enforce fairness constraints. While Random Forest doesn't have direct in-processing bias mitigation within AIF360, other models supported by AIF360 (e.g., adversarial debiasing for neural networks) could be considered if the model type is flexible.\n",
        "Post-processing Mitigation: Algorithms that adjust the model's predictions after the model has been trained. Examples include:\n",
        "\n",
        "    Equalized Odds Post-processing:\n",
        "            \n",
        "      Adjusting prediction thresholds for different groups to ensure that true positive rates and false positive rates are equal across groups. For our issue priority model, this would mean ensuring that the rate of correctly identifying\n",
        "      \n",
        "      \"High Priority\" issues is similar across all teams, and the rate of incorrectly labeling \"Low Priority\" issues as \"High Priority\" is also similar.\n",
        "      \n",
        "      Reject Option Classification: For predictions near the decision boundary, the model might defer to a \"fair\" option.\n",
        "\n",
        "    By integrating AIF360, the company can systematically identify and address biases, ensuring that the AI-driven issue prioritization is fair, equitable, and builds trust among all employees and stakeholders, rather than perpetuating existing inequalities. This commitment to fairness is not just ethical but also crucial for fostering a truly efficient and inclusive work environment.\n"
      ],
      "metadata": {
        "id": "zy-itXMOzWmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Task\n",
        "\n",
        "Innovation Challenge: Propose an AI tool to solve a software engineering problem not covered in class (e.g., automated documentation generation).\n",
        "\n",
        "Proposed AI Tool: \"DocuCoder - Intelligent Contextual Documentation Generator\"\n",
        "\n",
        "\n",
        "1. Tool's Purpose:\n",
        "\n",
        "    DocuCoder aims to solve the persistent problem of outdated, incomplete, or non-existent software documentation, specifically focusing on generating and maintaining high-quality, contextual documentation for codebases. Developers often neglect documentation due to time constraints, the fast pace of development, and the perceived drudgery of the task. Poor documentation hinders onboarding of new team members, complicates code maintenance, and impedes knowledge transfer. DocuCoder's primary purpose is to automatically generate and intelligently update various forms of documentation, including inline comments, function/module docstrings, API specifications, and high-level architectural summaries, significantly reducing the manual burden on developers while ensuring accuracy and relevance.\n",
        "\n",
        "2. Workflow:\n",
        "\n",
        "DocuCoder would integrate seamlessly into the existing developer workflow, primarily through IDE plugins, CI/CD pipelines, and version control hooks.\n",
        "\n",
        "    Initial Scan & Generation:\n",
        "        Upon integration, DocuCoder performs an initial scan of the codebase, analyzing the Abstract Syntax Tree (AST), variable names, function signatures, class structures, existing comments, and Git history.\n",
        "        It leverages a fine-tuned Large Language Model (LLM) to generate initial docstrings for functions, classes, and modules, based on their logical structure and perceived purpose.\n",
        "        For APIs, it can infer endpoints, parameters, and return types to generate OpenAPI/Swagger specifications.\n",
        "        For high-level documentation, it can summarize the interactions between different modules or services based on call graphs and dependencies.\n",
        "    Contextual Understanding & Semantic Analysis:\n",
        "        Beyond syntax, DocuCoder uses semantic analysis to understand the intent behind the code. This involves analyzing data flow, control flow, and external library usages.\n",
        "        It can identify \"business logic\" by recognizing patterns in data manipulation and decision-making structures, translating these into understandable natural language explanations.\n",
        "    Continuous Monitoring & Update:\n",
        "        Git Hooks Integration: DocuCoder integrates with Git hooks (e.g., pre-commit, post-merge) to detect code changes.\n",
        "        Intelligent Change Detection: When code is modified, DocuCoder intelligently analyzes the diff. If a function's parameters change, its return type is altered, or its internal logic is updated, DocuCoder automatically suggests updates to its corresponding docstring and related API documentation.\n",
        "        Feedback Loop: Developers can accept, modify, or reject AI-suggested documentation updates within their IDE. This feedback continually refines DocuCoder's understanding of the codebase and team's documentation style.\n",
        "        CI/CD Integration: DocuCoder can be integrated into the CI/CD pipeline to flag or even fail builds if critical code sections lack adequate documentation or if existing documentation becomes significantly out-of-sync with the code.\n",
        "    Query & Discovery:\n",
        "        A natural language interface allows developers to query the codebase, e.g., \"Explain how process_order function works,\" or \"What are the dependencies of the UserAuthService?\" DocuCoder will synthesize relevant documentation and code explanations.\n",
        "\n",
        "3. Impact:\n",
        "\n",
        "    Reduced Development Time: Significantly frees developers from the tedious task of manual documentation, allowing them to focus on core coding.\n",
        "    Improved Code Maintainability: Ensures documentation remains accurate and up-to-date with code changes, reducing technical debt.\n",
        "\n",
        "    Faster Onboarding: New team members can quickly understand complex codebases with comprehensive and current documentation.\n",
        "\n",
        "    Enhanced Knowledge Transfer: Critical system knowledge is captured and maintained automatically, mitigating risks associated with team member departures.\n",
        "\n",
        "    Higher Code Quality: The process of generating documentation can sometimes highlight logical flaws or areas for refactoring that might otherwise be missed.\n",
        "\n",
        "    Standardized Documentation: Helps enforce consistent documentation standards across a project or organization.\n",
        "    \n",
        "    Increased Developer Satisfaction: Reduces a commonly disliked task, leading to happier and more productive development teams.\n",
        "\n",
        "Differentiation from Existing Solutions:\n",
        "\n",
        "    While tools like Doxygen or Javadoc generate documentation from comments, and some LLMs can generate code snippets, DocuCoder's innovation lies in its proactive, intelligent, and continuous contextual documentation generation and updating, coupled with semantic understanding and a direct feedback loop within the developer's everyday tools. It moves beyond mere comment extraction to truly understand and explain the purpose and behavior of the code.\n"
      ],
      "metadata": {
        "id": "-hG6KeYk1QVL"
      }
    }
  ]
}